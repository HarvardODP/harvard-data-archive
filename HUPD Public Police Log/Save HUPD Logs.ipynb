{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a058c34f-607f-4a93-a5bb-829352640e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from typing import Iterable, Dict, Any, Optional\n",
    "\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901afe63-8389-4619-81e8-9dad3d997f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions\n",
    "# this works for Chrome and Windows.\n",
    "\n",
    "def system_downloads() -> Path:\n",
    "    return Path.home() / \"Downloads\"\n",
    "\n",
    "def make_chrome(headless: bool = True) -> webdriver.Chrome:\n",
    "    downloads_dir = str(system_downloads().resolve())\n",
    "    opts = Options()\n",
    "    prefs = {\n",
    "        \"plugins.always_open_pdf_externally\": True,\n",
    "        \"download.default_directory\": downloads_dir,\n",
    "        \"download.prompt_for_download\": False,\n",
    "        \"download.directory_upgrade\": True,\n",
    "        \"safebrowsing.enabled\": True,\n",
    "    }\n",
    "    opts.add_experimental_option(\"prefs\", prefs)\n",
    "    if headless:\n",
    "        opts.add_argument(\"--headless=new\")\n",
    "        opts.add_argument(\"--disable-gpu\")\n",
    "        opts.add_argument(\"--window-size=1920,1080\")\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=opts)\n",
    "    try:\n",
    "        driver.execute_cdp_cmd(\n",
    "            \"Page.setDownloadBehavior\",\n",
    "            {\"behavior\": \"allow\", \"downloadPath\": downloads_dir}\n",
    "        )\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    return driver\n",
    "\n",
    "# renames function to add (n) at the end should a file already exist\n",
    "def safe_rename(dest_dir: Path, filename: str) -> Path:\n",
    "    dest_dir.mkdir(parents=True, exist_ok=True)\n",
    "    base = Path(filename).stem\n",
    "    ext = Path(filename).suffix or \".pdf\"\n",
    "    candidate = dest_dir / f\"{base}{ext}\"\n",
    "    i = 1\n",
    "    while candidate.exists():\n",
    "        candidate = dest_dir / f\"{base} ({i}){ext}\"\n",
    "        i += 1\n",
    "    return candidate\n",
    "\n",
    "# check for newest pdf downloaded\n",
    "def newest_pdf_since(folder: Path, since_epoch: float) -> Optional[Path]:\n",
    "    pdfs = [p for p in folder.glob(\"*.pdf\") if p.stat().st_mtime >= since_epoch]\n",
    "    return max(pdfs, key=lambda p: p.stat().st_mtime) if pdfs else None\n",
    "\n",
    "# wait for download to finish\n",
    "def wait_for_download_in_downloads(timeout: int = 180, since_epoch: float = 0.0) -> Optional[Path]:\n",
    "    downloads = system_downloads()\n",
    "    end = time.time() + timeout\n",
    "    candidate = None\n",
    "    while time.time() < end:\n",
    "        cr_in_progress = list(downloads.glob(\"*.crdownload\"))\n",
    "        candidate = newest_pdf_since(downloads, since_epoch)\n",
    "        if candidate and not cr_in_progress:\n",
    "            # confirm size is stable\n",
    "            stable = 0\n",
    "            last_size = -1\n",
    "            for _ in range(6):\n",
    "                sz = candidate.stat().st_size\n",
    "                if sz == last_size and sz > 0:\n",
    "                    stable += 1\n",
    "                else:\n",
    "                    stable = 0\n",
    "                if stable >= 2:\n",
    "                    return candidate\n",
    "                last_size = sz\n",
    "                time.sleep(0.5)\n",
    "        time.sleep(0.5)\n",
    "    return None\n",
    "\n",
    "# moving the files\n",
    "def move_to_target(src_file: Path, target_dir: Path) -> Path:\n",
    "    dst = safe_rename(target_dir, src_file.name)\n",
    "    shutil.move(str(src_file), str(dst))\n",
    "    return dst\n",
    "\n",
    "# download the file process\n",
    "def download_direct(driver: webdriver.Chrome, url: str, target_dir: Path, timeout: int = 180) -> Dict[str, Any]:\n",
    "    t0 = time.time()\n",
    "    started_at = datetime.utcnow()\n",
    "    result = {\n",
    "        \"url\": url,\n",
    "        \"status\": \"500\",\n",
    "        \"filename\": None,\n",
    "        \"bytes\": None,\n",
    "        \"duration_s\": None,\n",
    "        \"saved_path\": None,\n",
    "        \"note\": \"\",\n",
    "        \"started_utc\": started_at.isoformat() + \"Z\",\n",
    "    }\n",
    "    try:\n",
    "        driver.get(url)\n",
    "        finished = wait_for_download_in_downloads(timeout=timeout, since_epoch=t0)\n",
    "        if not finished:\n",
    "            result[\"note\"] = \"no pdf in system downloads.\"\n",
    "            return result\n",
    "        moved = move_to_target(finished, target_dir)\n",
    "        result.update({\n",
    "            \"status\": \"200\",\n",
    "            \"filename\": moved.name,\n",
    "            \"bytes\": moved.stat().st_size,\n",
    "            \"duration_s\": round(time.time() - t0, 3),\n",
    "            \"saved_path\": str(moved),\n",
    "        })\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        result[\"note\"] = f\"{e.__class__.__name__}: {e}\"\n",
    "        return result\n",
    "\n",
    "# run a batch of csvs\n",
    "def run_batch(urls: Iterable[str], dest_dir: str, csv_out: str, headless: bool = True, per_timeout: int = 180) -> pd.DataFrame:\n",
    "    target_dir = Path(dest_dir).expanduser().resolve()\n",
    "    logs = []\n",
    "    driver = make_chrome(headless=headless)\n",
    "    try:\n",
    "        for url in urls:\n",
    "            logs.append(download_direct(driver, url, target_dir, timeout=per_timeout))\n",
    "    finally:\n",
    "        driver.quit()\n",
    "    df = pd.DataFrame(logs)\n",
    "    out = Path(csv_out).expanduser().resolve()\n",
    "    out.parent.mkdir(parents=True, exist_ok=True)\n",
    "    df.to_csv(out, index=False)\n",
    "    print(f\"saved log to: {out}\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4839fc-ed87-4456-8711-1e75779c63d4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "DEST_DIR = \"./2025\"\n",
    "CSV_OUT  = \"./download_status.csv\"\n",
    "\n",
    "# get list of dates - current year\n",
    "today = datetime.today()\n",
    "start = datetime(today.year, 1, 1)\n",
    "dates = [(start + timedelta(days=i)).strftime(\"%m%d%y\") for i in range((today - start).days + 1)]\n",
    "dates_short = [(start + timedelta(days=i)).strftime(\"%y-%m\") for i in range((today - start).days + 1)]\n",
    "\n",
    "# add to urls\n",
    "URLS = [f\"https://www.hupd.harvard.edu/sites/g/files/omnuum2276/files/20{dates_short[x]}/{dates[x]}.pdf\" for x in range(len(dates))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91de284-3071-47ca-b9c3-db6a077743e5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# run batch! you can adjust timeout time if needed; harvard secure will download each file in 1-3 seconds\n",
    "run_batch(URLS, dest_dir=DEST_DIR, csv_out=CSV_OUT, headless=True, per_timeout=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a6f53b-7167-45a7-8f11-2560cf048497",
   "metadata": {},
   "outputs": [],
   "source": [
    "# also this weird url thing where sometimes the last few dates of the current month have the url for the next month 2025-02 for 1/30 and 1/31\n",
    "# trying to adjust for that\n",
    "df = pd.read_csv(CSV_OUT)\n",
    "df_failed = df[df['status'] == 500]\n",
    "\n",
    "\n",
    "df_failed['dates'] = df_failed['url'].apply(lambda x: x[-10:-4])\n",
    "dates_alter = list(df_failed['dates'].unique())\n",
    "\n",
    "try_urls_short =[]\n",
    "\n",
    "for date in dates_alter:\n",
    "    next_month = str(int(date[:2])+1).zfill(2)\n",
    "    try_urls_short.append(f\"https://www.hupd.harvard.edu/sites/g/files/omnuum2276/files/20{date[-2:]}-{next_month}/{date}.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97437f71-97df-4674-988c-37217fa9b20b",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_batch(try_urls_short, dest_dir=DEST_DIR, csv_out=\"./try_download_status.csv\", headless=True, per_timeout=25)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
